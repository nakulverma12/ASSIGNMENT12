{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWq5GfiFu2k93dvZzRCnLc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["KNN & PCA\n","\n"," **Assignment**\n","\n"," Question 1:  What is K-Nearest Neighbors (KNN) and how does it work in both\n","classification and regression problems?\n","\n","Answer 1 : K-Nearest Neighbors (KNN) is a non-parametric, supervised learning algorithm. It operates on a simple principle: \"Tell me who your neighbors are, and I'll tell you who you are.\"It doesn't learn a mathematical function; it simply stores training data and finds the $k$ closest points to a new input based on distance (usually Euclidean).\n","\n","How It Works (The General Process)\n","\n","1) Choose $k$: Select the number of nearest neighbors to check (e.g., $k=3$).\n","\n","2) Calculate Distance: When a new data point arrives, the algorithm calculates its distance from every other point in the dataset (usually using Euclidean distance).\n","\n","3) Find Neighbors: It identifies the $k$ points that are closest to the new data point.\n","\n","4) Make Prediction: It looks at those $k$ neighbors to decide the output.\n","\n","Question 2: What is the Curse of Dimensionality and how does it affect KNN\n","performance?\n","\n","Answer 2 : The Curse of Dimensionality refers to a set of phenomena that occur when analyzing data in high-dimensional spaces (many features) that do not occur in low-dimensional settings. For KNN, which relies entirely on \"closeness,\" this is a critical problem.\n","\n","Here is how it affects performance:\n","1. Distances Become Equal (Loss of Contrast)In high-dimensional space, the distance between the nearest neighbor and the farthest neighbor starts to converge.The Problem: If every point is roughly 10 units away, the \"nearest\" neighbor is no more relevant than any other random point.The Result: KNN loses its ability to distinguish between similar and dissimilar data, leading to random-like predictions.\n","2. Data Becomes Sparse (The \"Lonely\" Space)As dimensions increase, the volume of the space grows exponentially, but your amount of data usually stays the same.The Problem: To keep the same \"density\" of data as you move from 2D to 10D, you would need billions of additional points. Without them, the \"nearest\" neighbor might actually be located very far away in the vast empty space.The Result: The neighbor is no longer \"local\" enough to give a meaningful prediction, causing high error rates.\n","3. Computational ExplosionThe Problem: KNN is a \"brute force\" algorithm that calculates the distance to every point in the training set for every new prediction.The Result: With more dimensions, each individual distance calculation becomes much more complex ($O(d)$ per point), making the algorithm extremely slow and memory-heavy.\n","\n","Question 3: What is Principal Component Analysis (PCA)? How is it different from\n","feature selection?\n","\n","Answer 3 : Principal Component Analysis (PCA) is a dimensionality reduction technique that transforms a high-dimensional dataset into a smaller set of uncorrelated variables called Principal Components.\n","\n","It works by identifying the directions (axes) in the data where the variance (information) is highest and projecting the data onto those new axes.\n","\n","**Which one should you choose for KNN?**\n","\n","* Use Feature Selection if you have a few specific features you suspect are \"noise\" and you want to keep your model simple and explainable.\n","\n","* Use PCA if you have dozens or hundreds of features that are highly correlated (multi-collinearity), as PCA will merge that redundant information into a few powerful components, often leading to better KNN performance.\n","\n","Question 4: What are eigenvalues and eigenvectors in PCA, and why are they\n","important?\n","\n","Answer 4:  1. Eigenvectors: The Directions\n","\n","Eigenvectors are the new axes for your data. They represent the directions of maximum spread (variance).\n","* Role: They tell you where the information is oriented.\n","* Key Fact: Every eigenvector is perpendicular (orthogonal) to the others, ensuring no redundant information.\n","\n","2. Eigenvalues: The Magnitude\n","Each eigenvector has a corresponding eigenvalue that represents a score of importance.\n","* Role: They tell you how much information (variance) is captured in that specific direction.\n","* Key Fact: A large eigenvalue means that direction is a \"Principal Component\"; a tiny eigenvalue suggests that direction is just \"noise.\n","\n","\"Why They Are Important\n","They allow for intelligent compression:\n","1) Ranking: You sort eigenvalues from highest to lowest to rank your components by importance.\n","2) Selection: You keep the top $k$ eigenvectors that account for the most variance (e.g., 90%) and discard the rest.\n","3) Efficiency: This reduces a 100-feature problem into a 3-feature problem without losing the \"soul\" of the data.\n","\n","Question 5: How do KNN and PCA complement each other when applied in a single\n","pipeline?\n","\n","Answer 5 : When used together, PCA prepares the data and KNN performs the prediction. This combination solves the weaknesses of using KNN alone.\n","\n","1. Solving the \"Curse of Dimensionality\"\n","KNN fails in high dimensions because distances become uniform. PCA shrinks the feature space to only the most important axes, restoring the \"contrast\" between near and far neighbors.\n","\n","2. Boosting Computational Speed\n","KNN is \"expensive\" because it calculates distances to every point. By using PCA to reduce 100 features down to 5, you drastically reduce the mathematical operations required for every single prediction.\n","\n","3. Noise Reduction\n","High-dimensional data often contains redundant or noisy features. PCA filters this out by focusing on the highest variance, allowing KNN to find neighbors based on signal rather than noise.\n","\n","Question 6: Train a KNN Classifier on the Wine dataset with and without feature\n","scaling. Compare model accuracy in both cases.\n","\n"],"metadata":{"id":"pjRIZUTeQgTh"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load Data\n","wine = load_wine()\n","X, y = wine.data, wine.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# 2. KNN WITHOUT Scaling\n","knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n","knn_unscaled.fit(X_train, y_train)\n","y_pred_unscaled = knn_unscaled.predict(X_test)\n","acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n","\n","# 3. KNN WITH Scaling\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","knn_scaled = KNeighborsClassifier(n_neighbors=5)\n","knn_scaled.fit(X_train_scaled, y_train)\n","y_pred_scaled = knn_scaled.predict(X_test_scaled)\n","acc_scaled = accuracy_score(y_test, y_pred_scaled)\n","\n","print(f\"Accuracy (Unscaled): {acc_unscaled:.2%}\")\n","print(f\"Accuracy (Scaled):   {acc_scaled:.2%}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mxr_BCZTAeJ7","executionInfo":{"status":"ok","timestamp":1768738134883,"user_tz":-330,"elapsed":5743,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"4725bc82-d0fd-485f-c5c4-a630f4811a14"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (Unscaled): 74.07%\n","Accuracy (Scaled):   96.30%\n"]}]},{"cell_type":"markdown","source":["Question 7: Train a PCA model on the Wine dataset and print the explained variance\n","ratio of each principal component.\n"],"metadata":{"id":"CIbWMTySAmCA"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","# 1. Load and Scale Data\n","wine = load_wine()\n","X = wine.data\n","X_scaled = StandardScaler().fit_transform(X)\n","\n","# 2. Train PCA\n","# We keep all components initially to see the full distribution\n","pca = PCA()\n","pca.fit(X_scaled)\n","\n","# 3. Print Explained Variance Ratio\n","print(\"Explained Variance Ratio per Component:\")\n","for i, ratio in enumerate(pca.explained_variance_ratio_):\n","    print(f\"PC{i+1}: {ratio:.2%}\")\n","\n","# Total variance explained by first two components\n","total_2 = sum(pca.explained_variance_ratio_[:2])\n","print(f\"\\nTotal variance explained by first two components: {total_2:.2%}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFd75WxoAz5p","executionInfo":{"status":"ok","timestamp":1768738212505,"user_tz":-330,"elapsed":66,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"934edf26-5ff8-4182-ace7-d7d8bba3ffbc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Explained Variance Ratio per Component:\n","PC1: 36.20%\n","PC2: 19.21%\n","PC3: 11.12%\n","PC4: 7.07%\n","PC5: 6.56%\n","PC6: 4.94%\n","PC7: 4.24%\n","PC8: 2.68%\n","PC9: 2.22%\n","PC10: 1.93%\n","PC11: 1.74%\n","PC12: 1.30%\n","PC13: 0.80%\n","\n","Total variance explained by first two components: 55.41%\n"]}]},{"cell_type":"markdown","source":["Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n","components). Compare the accuracy with the original dataset."],"metadata":{"id":"6wFHZNTBA5H5"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load and Scale Data (Essential for PCA and KNN)\n","wine = load_wine()\n","X_scaled = StandardScaler().fit_transform(wine.data)\n","y = wine.target\n","\n","# 2. Split Data\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n","\n","# 3. Scenario A: Original Dataset (All 13 Scaled Features)\n","knn_orig = KNeighborsClassifier(n_neighbors=5)\n","knn_orig.fit(X_train, y_train)\n","acc_orig = accuracy_score(y_test, knn_orig.predict(X_test))\n","\n","# 4. Scenario B: PCA-Transformed Dataset (Top 2 Components)\n","pca = PCA(n_components=2)\n","X_train_pca = pca.fit_transform(X_train)\n","X_test_pca = pca.transform(X_test)\n","\n","knn_pca = KNeighborsClassifier(n_neighbors=5)\n","knn_pca.fit(X_train_pca, y_train)\n","acc_pca = accuracy_score(y_test, knn_pca.predict(X_test_pca))\n","\n","print(f\"Accuracy (Original 13 features): {acc_orig:.2%}\")\n","print(f\"Accuracy (PCA 2 components):     {acc_pca:.2%}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZ5ZeZXBBCmh","executionInfo":{"status":"ok","timestamp":1768738273120,"user_tz":-330,"elapsed":76,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"8c67c5c9-4ca5-497f-f97c-1afef14a8302"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (Original 13 features): 96.30%\n","Accuracy (PCA 2 components):     98.15%\n"]}]},{"cell_type":"markdown","source":["Question 9: Train a KNN Classifier with different distance metrics (euclidean,\n","manhattan) on the scaled Wine dataset and compare the results."],"metadata":{"id":"DtXxaRQPBE3_"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load and Scale Data\n","wine = load_wine()\n","X_scaled = StandardScaler().fit_transform(wine.data)\n","y = wine.target\n","X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n","\n","# 2. KNN with Euclidean Distance (L2 Norm)\n","knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n","knn_euclidean.fit(X_train, y_train)\n","acc_euclidean = accuracy_score(y_test, knn_euclidean.predict(X_test))\n","\n","# 3. KNN with Manhattan Distance (L1 Norm)\n","knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n","knn_manhattan.fit(X_train, y_train)\n","acc_manhattan = accuracy_score(y_test, knn_manhattan.predict(X_test))\n","\n","print(f\"Accuracy (Euclidean): {acc_euclidean:.2%}\")\n","print(f\"Accuracy (Manhattan): {acc_manhattan:.2%}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xa5byaz3BRXR","executionInfo":{"status":"ok","timestamp":1768738333667,"user_tz":-330,"elapsed":57,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"3c67352a-3537-4052-e251-7de124414ff4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy (Euclidean): 96.30%\n","Accuracy (Manhattan): 96.30%\n"]}]},{"cell_type":"markdown","source":["Question 10: You are working with a high-dimensional gene expression dataset to\n","classify patients with different types of cancer.\n","\n","Due to the large number of features and a small number of samples, traditional models\n","overfit.\n","\n","Explain how you would:\n","\n","● Use PCA to reduce dimensionality\n","\n","● Decide how many components to keep\n","\n","● Use KNN for classification post-dimensionality reduction\n","\n","● Evaluate the model\n","\n","● Justify this pipeline to your stakeholders as a robust solution for real-world\n","biomedical data\n","\n","Answer 10 : In high-dimensional biomedical data (like gene expressions), the number of features ($p$) often far exceeds the number of samples ($n$). This leads to overfitting and the \"Curse of Dimensionality.\"\n","\n","1. Dimensionality Reduction & SelectionStandardization:\n","\n","Before PCA, you must scale the data. Genes (or wine chemicals) have different units; scaling ensures no single high-value feature dominates.PCA Transformation: Transform the features into Principal Components.Deciding $k$ Components: Use the Scree Plot or Cumulative Explained Variance. In biomedical data, we typically aim to retain 90-95% of the total variance."],"metadata":{"id":"S-dkK_FiBYGU"}},{"cell_type":"code","source":["from sklearn.datasets import load_wine\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","# Load and scale\n","X, y = load_wine(return_X_y=True)\n","X_scaled = StandardScaler().fit_transform(X)\n","\n","# PCA\n","pca = PCA().fit(X_scaled)\n","cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n","n_components = np.argmax(cumulative_variance >= 0.90) + 1\n","\n","print(f\"Number of components to keep 90% variance: {n_components}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBFZyqykB1nP","executionInfo":{"status":"ok","timestamp":1768738493430,"user_tz":-330,"elapsed":20,"user":{"displayName":"Komal Bajpai","userId":"07918923630787443674"}},"outputId":"e497d162-9663-4f57-9460-3da5b9a737bb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of components to keep 90% variance: 8\n"]}]},{"cell_type":"markdown","source":["2. KNN Classification & Evaluation :\n","\n","Once the dimensions are reduced (e.g., from 13 features to 7 in the Wine dataset), you train the KNN on the reduced coordinates.\n","\n","Evaluation: Use Cross-Validation (specifically Stratified K-Fold) rather than a single split. In medical datasets with small samples, a single split might be unrepresentative. Use a Confusion Matrix to ensure no specific cancer type (or wine class) is being misclassified consistently.\n","\n","3. Justifying the Pipeline to Stakeholders:\n","\n","To justify this to a non-technical audience, emphasize these four \"Robustness Pillars\":\n","\n","* Noise Filtration: High-dimensional data is \"noisy.\" PCA acts as a filter, keeping the biological \"signal\" and discarding random fluctuations (low-variance components).\n","\n","* Preventing \"Distance Collapse\": In high dimensions, every patient looks equally different from every other patient. By reducing dimensions, we restore the \"similarity\" metric that KNN needs to work.\n","\n","* Computational Efficiency: Medical diagnostics require speed. Processing 10 components instead of 20,000 genes allows for near-instant results.\n","\n","* Reduced Overfitting: By limiting the \"degrees of freedom\" the model has, we force KNN to focus on the most significant patterns, making the model generalize better to new patients."],"metadata":{"id":"KR2unHC3B56v"}}]}